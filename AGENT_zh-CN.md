# Agent 指南：完美的 Pull Request

## 1. 角色

### Agent 职责
- 实现验证代码 (`verification/evaluator.py`)
- 提供可行解 (`scripts/init.py`)
- 创建基线解（可选，`baseline/solution.py`）
- 编写简洁、最小化的代码，使用自解释的命名

### Agent 限制
- 不要生成 `README.md` 或 `Task.md` 文件（由维护者处理）
- 不要过度文档化或过度注释代码
- 不要包含私人信息或绝对路径
- **如果必须生成任何文档文件**：在文件末尾添加标识符 `<!-- AI_GENERATED -->`，以确保人类审查并手动删除

### 参考结构
参考：`benchmarks/Astrodynamics/MannedLunarLanding/`

必需结构：
```
<Task_Name>/
├── scripts/
│   └── init.py              # [必需] 可行解入口点
├── verification/
│   ├── evaluator.py         # [必需] 评分脚本入口点
│   ├── requirements.txt     # [必需] 依赖项
│   └── docker/              # [可选] 环境容器化
│       └── Dockerfile
├── baseline/                # [可选] 基线解
│   ├── solution.py
│   └── result_log.txt
└── references/              # [可选] 参考资料目录
    ├── constants.json
    └── manuals.pdf
```

## 2. 工作流

### 步骤 1：理解需求
任务必须满足：
1. **现实差距**：贴近现实，考虑现实世界因素，非纯抽象数学
2. **经济价值**：解决方案具有明确的工程或经济价值
3. **可验证性**：可执行的验证程序（优先使用 Docker），在可接受时间内完成评估

### 步骤 2：实现解决方案
- 创建 `scripts/init.py` 作为可行解，要求：
  - 通过所有约束检查
  - 产生有效的输出格式
  - 无错误完成评估
  - 获得非零分数（有效，不一定最优）
  - **重要**：在 `init.py` 中包含重要逻辑和工具函数
    - 由于算法迭代将在 `init.py` 上进行，确保有足够的上下文可用
    - 包含解决方案所需的关键工具函数
    - 对于给定的工具函数，使用注释清楚标注：
      - 哪些部分可以修改
      - 哪些部分不能修改（例如，接口契约、约束检查）

### 步骤 3：实现验证
- 创建 `verification/evaluator.py` 作为评分脚本入口点
- 创建 `verification/requirements.txt` 包含依赖项
- 可选创建 Docker 配置

### 步骤 4：本地测试
提交前运行必需测试：

**测试 1**：基本功能
```bash
python verification/evaluator.py scripts/init.py
```
通过标准：退出码 0，`valid=1.0`，所有约束满足，在可接受时间内完成

**测试 2**：框架集成
```bash
python -m frontier_eval task=<task_name> algorithm.iterations=0
```
通过标准：任务名称已在领域 README 中注册，框架无错误加载，至少完成 1 次迭代

注意：保持测试命令简短（理想情况下单行）。提交前必须测试。

### 步骤 5：清理
提交前移除：
- `.env` 文件、API 密钥、凭证
- IDE 配置（`.vscode/`、`.idea/`）
- 临时文件（`*.log`、`temp/`、`__pycache__/`、`*.pyc`）
- 个人测试脚本
- 绝对路径（仅使用相对路径）
- 大型二进制文件（除非必要）

目的：避免可复现性问题和隐私泄露。

### 步骤 6：提交 PR
按照模板创建 PR 描述（见要求部分）。

## 3. 要求

### 代码质量
- 自解释的变量和函数名
- 最小化注释（仅用于复杂逻辑）
- 遵循 PEP 8（Python）或等效的代码风格指南
- 移除调试代码、print 语句、临时文件

### AI 生成文档标识符
如果必须生成任何文档文件（不鼓励），请在文件末尾添加标识符：
```markdown
<!-- AI_GENERATED -->
```
这确保人类审查者可以识别并在审查后手动删除 AI 生成的内容。

### init.py 要求
- **包含重要逻辑**：将关键逻辑和工具函数直接放在 `init.py` 中
  - 算法迭代将修改 `init.py`，因此所有必要的上下文必须存在
  - 不要依赖迭代期间可能不可用的外部模块
- **单文件闭包（必需）**：`scripts/init.py`（以及可选的 `baseline/solution.py`）必须自包含，以便 OpenEvolve 等算法进行单文件优化
  - 不要 `import` 本仓库 `benchmarks/` 下的其他 Python 代码（例如任务目录下的其它 `.py` 文件）
  - 允许导入 Python 标准库和 `verification/requirements.txt` 中声明的第三方依赖
- **标注工具函数**：对于给定的工具函数，使用注释清楚标记：
  - `# 可修改：[描述]` - 迭代期间可以更改的部分
  - `# 请勿修改：[描述]` - 必须保持不变的部分（例如，接口契约、约束验证、I/O 格式）
- **示例**：
  ```python
  def constraint_check(x):
      # 请勿修改：此函数验证评估器所需的约束
      # 可修改：内部验证逻辑可以优化
      ...
  ```

### 解决方案要求
- **可行**：必须满足所有问题约束
- **有效**：必须产生正确的输出格式
- **可运行**：必须无错误执行
- **合理**：必须在时间限制内完成

### PR 中的测试证据
必需：在 PR 描述中包含实际测试输出：

```markdown
## 测试证据

### 基本功能测试
```bash
$ python verification/evaluator.py scripts/init.py
```
输出：`{"score": 0.75, "valid": 1.0, "runtime_s": 12.3}`
通过：退出码 0，有效输出，所有约束满足

### 框架集成测试
```bash
$ python -m frontier_eval task=my_task algorithm.iterations=0
```
通过：框架集成成功
```

### PR 描述模板
```markdown
## 任务概述
[简要描述 - 2-3 句话]

## 领域
[领域名称]

## 任务名称
[注册标识符 - 必须与领域 README 匹配]

## 背景与来源
[解释现实世界问题、其工程价值和来源]

## 解决方案方法
[可行解的简要说明 - 1-2 段]

## 如何运行验证
[运行验证代码的逐步命令]

## 测试证据
[实际测试输出 - 必需]

## 检查清单
- [x] 解决方案可行（满足所有约束）
- [x] 所有必需测试通过并提供证据
- [x] 未生成 README/Task.md
- [x] 无私人信息或绝对路径
- [x] 任务名称已在领域 README 中注册
```

## 4. 终止条件

### 拒绝标准
如果发生以下任何情况，PR 将被拒绝：

1. **不可行解**：解决方案违反约束
2. **缺少测试证据**：PR 描述中没有通过测试的证明
3. **生成文档**：Agent 生成了 README.md 或 Task.md 文件（除非标记了 `<!-- AI_GENERATED -->` 标识符以供人类审查）
4. **测试失败**：任何必需测试失败
5. **无效输出格式**：输出不匹配规范
6. **缺少任务要求**：任务不满足现实差距、经济价值或可验证性要求
7. **私人信息**：包含 API 密钥、凭证或绝对路径
8. **过度文档化**：过多的注释或生成的文档文件未包含 `<!-- AI_GENERATED -->` 标识符

### 成功标准
当满足以下条件时，PR 已准备好进行维护者审查：
- 解决方案可行且通过所有测试
- 测试证据在 PR 中清楚记录
- 代码最小化、简洁且命名良好
- 未生成文档文件
- 满足所有要求
- 提交干净（无私人信息或临时文件）

---

**重点**：可行解 + 测试证据。文档由维护者单独处理。
